
# Scrap Tool

Scrap is a Bash-based script designed to extract specific information from a webpage's HTML using tools like `curl`, `grep`, and `sed`. It demonstrates the fundamentals of web scraping using shell scripting.

---

## Project Structure

The project includes the following files:

1. **`scrap.sh`**: The primary script responsible for fetching and parsing HTML content.
2. **`debug.html`**: A sample HTML file used for testing and debugging (generated by the script).

---

## How to Use Scrap

### 1. Prerequisites
- Ensure you have Bash installed.
- Install `GNU grep` (required for advanced regular expressions):
  ```bash
  brew install grep
  ```
  On macOS, use `ggrep` instead of `grep` for this script.

### 2. Make the Script Executable
Run the following command to make the script executable:
```bash
chmod +x scrap.sh
```

### 3. Run the Script
Execute the script using:
```bash
./scrap.sh
```

### 4. Output
- The script fetches the webpage content and saves it to `debug.html`.
- It extracts and displays laptop information (name, description, and price) in the terminal, one per line.

---

## Script Overview

```bash
#!/bin/bash

# Step 1: Fetch the HTML content
URL="https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops"
curl -s "$URL" > debug.html

# Debug: Check if debug.html has content
if [ ! -s debug.html ]; then
    echo "Error: debug.html is empty. Check your curl command."
    exit 1
fi

# Step 2: Extract and print laptop details directly
echo "Extracting laptop details..."

# Extract names (grab everything between <a ... class="title"> and </a>)
names=$(ggrep -oP '<a [^>]*class="title" [^>]*>.*?</a>' debug.html | sed -E 's/<a [^>]*class="title" [^>]*>(.*)<\/a>/\1/')

# Extract descriptions (grab everything between <p class="description card-text"> and </p>)
descriptions=$(ggrep -oP '<p class="description card-text">.*?</p>' debug.html | sed -E 's/<p class="description card-text">(.*)<\/p>/\1/')

# Extract prices (grab everything between <h4 class="price ..."> and </h4>)
prices=$(ggrep -oP '<h4 class="price [^>]*>.*?</h4>' debug.html | sed -E 's/<h4 class="price [^>]*>(.*)<\/h4>/\1/')

# Decode HTML entities (e.g., &quot; -> ")
decode_html_entities() {
    echo "$1" | sed 's/&quot;/"/g'
}

# Combine extracted data into a formatted output
paste <(echo "$names") <(echo "$descriptions") <(echo "$prices") | while IFS=$'\t' read -r name description price; do
    # Decode HTML entities for readability
    name=$(decode_html_entities "$name")
    description=$(decode_html_entities "$description")
    price=$(decode_html_entities "$price")

    if [ -n "$name" ] && [ -n "$description" ] && [ -n "$price" ]; then
        echo "$name | $description | $price"
    else
        echo "Incomplete data for one laptop. Skipping..."
    fi

done
```

---

## Example Interaction

### Script Execution
```bash
$ ./scrap.sh
Extracting laptop details...
Asus VivoBook... | Asus VivoBook X441NA-GA190 Chocolate Black, 14", Celeron N3450, 4GB, 128GB SSD, Endless OS, ENG kbd | $295.99
Prestigio Smar... | Prestigio SmartBook 133S Dark Grey, 13.3" FHD IPS, Celeron N3350 1.1GHz, 4GB, 32GB, Windows 10 Pro + Office 365 1 gadam | $299
Acer Aspire ES... | Acer Aspire ES1-572 Black, 15.6" HD, Core i3-6006U, 4GB, 128GB SSD, Linux | $379.95
```

---

## Notes

1. **Dependencies**:
   - `curl`: Fetches webpage content.
   - `ggrep`: Parses HTML using Perl-compatible regular expressions.
   - `sed`: Extracts specific lines.

2. **File Requirements**:
   - The `debug.html` file will be overwritten each time the script runs.

3. **Error Handling**:
   - If `curl` fails to fetch the content, the script notifies the user.
   - If no laptop data is found, it suggests checking the webpage structure.

---


```

